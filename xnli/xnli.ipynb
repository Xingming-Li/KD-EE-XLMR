{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100","authorship_tag":"ABX9TyP6a10agI/Oy9wordZeNS7K"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X5MiQGC_OIvW","executionInfo":{"status":"ok","timestamp":1758618276260,"user_tz":-120,"elapsed":18457,"user":{"displayName":"Xingming Li","userId":"09069172943603800000"}},"outputId":"6311e28d-debf-4177-a32b-5b5f5050fb8d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install -r /content/drive/MyDrive/thesis/xnli/requirements.txt  # For XNLI\n","\n","# For correct version of transformers\n","!pip uninstall transformers -y\n","!pip install git+https://github.com/huggingface/transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jOpoDCHetMVT","executionInfo":{"status":"ok","timestamp":1758618321572,"user_tz":-120,"elapsed":43545,"user":{"displayName":"Xingming Li","userId":"09069172943603800000"}},"outputId":"57b76a37-e9b1-4127-97db-86cce38a47f0"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: accelerate>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from -r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 1)) (1.10.1)\n","Requirement already satisfied: datasets>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from -r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 2)) (4.0.0)\n","Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.12/dist-packages (from -r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 3)) (0.2.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from -r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 4)) (1.16.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from -r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 5)) (1.6.1)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from -r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 6)) (5.29.5)\n","Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.12/dist-packages (from -r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 7)) (2.8.0+cu126)\n","Collecting evaluate (from -r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 8))\n","  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n","Collecting fvcore (from -r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 9))\n","  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from -r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 10)) (0.6.2)\n","Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.12.0->-r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 1)) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.12.0->-r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 1)) (25.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.12.0->-r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 1)) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.12.0->-r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 1)) (6.0.2)\n","Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.12.0->-r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 1)) (0.35.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=1.8.0->-r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 2)) (3.19.1)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=1.8.0->-r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 2)) (18.1.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=1.8.0->-r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 2)) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets>=1.8.0->-r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 2)) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets>=1.8.0->-r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 2)) (2.32.4)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets>=1.8.0->-r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 2)) (4.67.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=1.8.0->-r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 2)) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=1.8.0->-r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 2)) (0.70.16)\n","Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=1.8.0->-r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 2)) (2025.3.0)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 5)) (1.5.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 5)) (3.6.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->-r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 7)) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->-r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 7)) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->-r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 7)) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->-r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 7)) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->-r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 7)) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->-r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 7)) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->-r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 7)) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->-r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 7)) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->-r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 7)) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->-r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 7)) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->-r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 7)) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->-r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 7)) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->-r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 7)) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->-r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 7)) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->-r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 7)) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->-r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 7)) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->-r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 7)) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->-r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 7)) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->-r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 7)) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.3->-r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 7)) (3.4.0)\n","Collecting yacs>=0.1.6 (from fvcore->-r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 9))\n","  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n","Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.12/dist-packages (from fvcore->-r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 9)) (3.1.0)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from fvcore->-r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 9)) (11.3.0)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from fvcore->-r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 9)) (0.9.0)\n","Collecting iopath>=0.1.7 (from fvcore->-r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 9))\n","  Downloading iopath-0.1.10.tar.gz (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=1.8.0->-r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 2)) (3.12.15)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate>=0.12.0->-r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 1)) (1.1.10)\n","Collecting portalocker (from iopath>=0.1.7->fvcore->-r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 9))\n","  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=1.8.0->-r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 2)) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=1.8.0->-r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 2)) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=1.8.0->-r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 2)) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=1.8.0->-r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 2)) (2025.8.3)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.3->-r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 7)) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.3->-r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 7)) (3.0.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=1.8.0->-r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 2)) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=1.8.0->-r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 2)) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=1.8.0->-r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 2)) (2025.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=1.8.0->-r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 2)) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=1.8.0->-r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 2)) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=1.8.0->-r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 2)) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=1.8.0->-r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 2)) (1.7.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=1.8.0->-r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 2)) (6.6.4)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=1.8.0->-r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 2)) (0.3.2)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=1.8.0->-r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 2)) (1.20.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=1.8.0->-r /content/drive/MyDrive/thesis/xnli/requirements.txt (line 2)) (1.17.0)\n","Downloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading yacs-0.1.8-py3-none-any.whl (14 kB)\n","Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n","Building wheels for collected packages: fvcore, iopath\n","  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61397 sha256=06afa00d1046595e9072b901741dcddda14dce8027f558c16f8f039594e9522b\n","  Stored in directory: /root/.cache/pip/wheels/ed/9f/a5/e4f5b27454ccd4596bd8b62432c7d6b1ca9fa22aef9d70a16a\n","  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31527 sha256=cc6bc527bf7ab35ef633e6c3ae423e2cc49773b2ad7b4808f943148f38824ead\n","  Stored in directory: /root/.cache/pip/wheels/7c/96/04/4f5f31ff812f684f69f40cb1634357812220aac58d4698048c\n","Successfully built fvcore iopath\n","Installing collected packages: yacs, portalocker, iopath, fvcore, evaluate\n","Successfully installed evaluate-0.4.6 fvcore-0.1.5.post20221221 iopath-0.1.10 portalocker-3.2.0 yacs-0.1.8\n","Found existing installation: transformers 4.56.1\n","Uninstalling transformers-4.56.1:\n","  Successfully uninstalled transformers-4.56.1\n","Collecting git+https://github.com/huggingface/transformers\n","  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-qq0q4398\n","  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-qq0q4398\n","  Resolved https://github.com/huggingface/transformers to commit cbb290ec23ccd9b5c1d1ff4d333477449891debb\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.0.dev0) (3.19.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.0.dev0) (0.35.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.0.dev0) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.0.dev0) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.0.dev0) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.0.dev0) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.0.dev0) (2.32.4)\n","Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.0.dev0) (0.22.0)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.0.dev0) (0.6.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.0.dev0) (4.67.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.57.0.dev0) (2025.3.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.57.0.dev0) (4.15.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.57.0.dev0) (1.1.10)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.57.0.dev0) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.57.0.dev0) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.57.0.dev0) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.57.0.dev0) (2025.8.3)\n","Building wheels for collected packages: transformers\n","  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for transformers: filename=transformers-4.57.0.dev0-py3-none-any.whl size=11477748 sha256=75a0b52a92728da1053c03f65325e48b2ec3785349a6e7dc61109471b0027b8c\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-mcbetgxu/wheels/49/a7/50/c9fdabbf10e51bb1256adb0c1a587fedd7184f5bad28d47fe3\n","Successfully built transformers\n","Installing collected packages: transformers\n","Successfully installed transformers-4.57.0.dev0\n"]}]},{"cell_type":"code","source":["# Fine-tuning\n","!python /content/drive/MyDrive/thesis/xnli/run_xnli_eff.py \\\n","  --model_name_or_path /content/drive/MyDrive/thesis/distilled_models/l7/e1 \\\n","  --language en \\\n","  --train_language en \\\n","  --do_train \\\n","  --per_device_train_batch_size 128 \\\n","  --learning_rate 7e-5 \\\n","  --num_train_epochs 2 \\\n","  --max_seq_length 128 \\\n","  --output_dir /content/drive/MyDrive/thesis/xnli/distilled/l7_ft/e1_ee_test \\\n","  --save_steps -1 \\\n","  --early_exit True"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fW6MgaXOQqox","outputId":"aa1b4e64-7446-4454-900e-fd88fb3c7452","executionInfo":{"status":"ok","timestamp":1758618535379,"user_tz":-120,"elapsed":147508,"user":{"displayName":"Xingming Li","userId":"09069172943603800000"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:__main__:Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","INFO:__main__:Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","average_tokens_across_devices=True,\n","batch_eval_metrics=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_persistent_workers=False,\n","dataloader_pin_memory=True,\n","dataloader_prefetch_factor=None,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","do_eval=False,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_do_concat_batches=True,\n","eval_on_start=False,\n","eval_steps=None,\n","eval_strategy=IntervalStrategy.NO,\n","eval_use_gather_object=False,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=None,\n","hub_revision=None,\n","hub_strategy=HubStrategy.EVERY_SAVE,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_for_metrics=[],\n","include_inputs_for_metrics=False,\n","include_num_input_tokens_seen=no,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=7e-05,\n","length_column_name=length,\n","liger_kernel_config=None,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=/content/drive/MyDrive/thesis/xnli/distilled/l7_ft/e1_ee_test/runs/Sep23_09-06-48_83d6e96eeec3,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=IntervalStrategy.STEPS,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=SchedulerType.LINEAR,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_train_epochs=2.0,\n","optim=OptimizerNames.ADAMW_TORCH_FUSED,\n","optim_args=None,\n","optim_target_modules=None,\n","output_dir=/content/drive/MyDrive/thesis/xnli/distilled/l7_ft/e1_ee_test,\n","overwrite_output_dir=False,\n","parallelism_config=None,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=128,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard', 'wandb'],\n","restore_callback_states_from_checkpoint=False,\n","resume_from_checkpoint=None,\n","run_name=None,\n","save_on_each_node=False,\n","save_only_model=False,\n","save_safetensors=True,\n","save_steps=-1.0,\n","save_strategy=SaveStrategy.STEPS,\n","save_total_limit=None,\n","seed=42,\n","skip_memory_metrics=True,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torch_empty_cache_steps=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_legacy_prediction_loop=False,\n","use_liger_kernel=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","\rREADME.md: 0.00B [00:00, ?B/s]\rREADME.md: 20.8kB [00:00, 69.3MB/s]\n","Generating dataset xnli (/root/.cache/huggingface/datasets/xnli/en/0.0.0/b8dd5d7af51114dbda02c0e3f6133f332186418e)\n","INFO:datasets.builder:Generating dataset xnli (/root/.cache/huggingface/datasets/xnli/en/0.0.0/b8dd5d7af51114dbda02c0e3f6133f332186418e)\n","Downloading and preparing dataset xnli/en to /root/.cache/huggingface/datasets/xnli/en/0.0.0/b8dd5d7af51114dbda02c0e3f6133f332186418e...\n","INFO:datasets.builder:Downloading and preparing dataset xnli/en to /root/.cache/huggingface/datasets/xnli/en/0.0.0/b8dd5d7af51114dbda02c0e3f6133f332186418e...\n","en/train-00000-of-00001.parquet: 100% 50.2M/50.2M [00:00<00:00, 61.5MB/s]\n","en/test-00000-of-00001.parquet: 100% 308k/308k [00:00<00:00, 672kB/s]\n","en/validation-00000-of-00001.parquet: 100% 157k/157k [00:00<00:00, 602kB/s]\n","Downloading took 0.0 min\n","INFO:datasets.download.download_manager:Downloading took 0.0 min\n","Checksum Computation took 0.0 min\n","INFO:datasets.download.download_manager:Checksum Computation took 0.0 min\n","Generating train split\n","INFO:datasets.builder:Generating train split\n","Generating train split: 100% 392702/392702 [00:00<00:00, 1194700.13 examples/s]\n","Generating test split\n","INFO:datasets.builder:Generating test split\n","Generating test split: 100% 5010/5010 [00:00<00:00, 1050148.08 examples/s]\n","Generating validation split\n","INFO:datasets.builder:Generating validation split\n","Generating validation split: 100% 2490/2490 [00:00<00:00, 759771.35 examples/s]\n","All the splits matched successfully.\n","INFO:datasets.utils.info_utils:All the splits matched successfully.\n","Dataset xnli downloaded and prepared to /root/.cache/huggingface/datasets/xnli/en/0.0.0/b8dd5d7af51114dbda02c0e3f6133f332186418e. Subsequent calls will reuse this data.\n","INFO:datasets.builder:Dataset xnli downloaded and prepared to /root/.cache/huggingface/datasets/xnli/en/0.0.0/b8dd5d7af51114dbda02c0e3f6133f332186418e. Subsequent calls will reuse this data.\n","[INFO|configuration_utils.py:757] 2025-09-23 09:06:55,789 >> loading configuration file /content/drive/MyDrive/thesis/distilled_models/l7/e1/config.json\n","[INFO|configuration_utils.py:833] 2025-09-23 09:06:55,794 >> Model config XLMRobertaConfig {\n","  \"architectures\": [\n","    \"XLMRobertaModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"dtype\": \"float32\",\n","  \"eos_token_id\": 2,\n","  \"finetuning_task\": \"xnli\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"entailment\",\n","    \"1\": \"neutral\",\n","    \"2\": \"contradiction\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"contradiction\": 2,\n","    \"entailment\": 0,\n","    \"neutral\": 1\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"xlm-roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_hidden_states\": true,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.57.0.dev0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250002\n","}\n","\n","[INFO|tokenization_utils_base.py:2057] 2025-09-23 09:06:56,297 >> loading file sentencepiece.bpe.model\n","[INFO|tokenization_utils_base.py:2057] 2025-09-23 09:06:56,297 >> loading file tokenizer.json\n","[INFO|tokenization_utils_base.py:2057] 2025-09-23 09:06:56,297 >> loading file added_tokens.json\n","[INFO|tokenization_utils_base.py:2057] 2025-09-23 09:06:56,297 >> loading file special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2057] 2025-09-23 09:06:56,297 >> loading file tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2057] 2025-09-23 09:06:56,297 >> loading file chat_template.jinja\n","[INFO|configuration_utils.py:757] 2025-09-23 09:07:00,310 >> loading configuration file /content/drive/MyDrive/thesis/distilled_models/l7/e1/config.json\n","[INFO|configuration_utils.py:833] 2025-09-23 09:07:00,311 >> Model config XLMRobertaConfig {\n","  \"architectures\": [\n","    \"XLMRobertaModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"dtype\": \"float32\",\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"xlm-roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_hidden_states\": true,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.57.0.dev0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250002\n","}\n","\n","[INFO|modeling_utils.py:1202] 2025-09-23 09:07:00,327 >> loading weights file /content/drive/MyDrive/thesis/distilled_models/l7/e1/model.safetensors\n","[INFO|modeling_utils.py:5572] 2025-09-23 09:07:10,672 >> All model checkpoint weights were used when initializing XLMRobertaModel.\n","\n","[INFO|modeling_utils.py:5580] 2025-09-23 09:07:10,672 >> All the weights of XLMRobertaModel were initialized from the model checkpoint at /content/drive/MyDrive/thesis/distilled_models/l7/e1.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaModel for predictions without further training.\n","WARNING:__main__:⚠️ No exit_heads found, using random initialization\n","Running tokenizer on train dataset:   0% 0/392702 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/xnli/en/0.0.0/b8dd5d7af51114dbda02c0e3f6133f332186418e/cache-537d2f9e9bd4ef58.arrow\n","INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/xnli/en/0.0.0/b8dd5d7af51114dbda02c0e3f6133f332186418e/cache-537d2f9e9bd4ef58.arrow\n","Running tokenizer on train dataset: 100% 392702/392702 [00:32<00:00, 12041.47 examples/s]\n","INFO:__main__:Sample 335243 of the training set: {'premise': \"you know when their parents come and it 's hard to get them out and a lot of parents have places to go and and things like that and it 's late at night so\", 'hypothesis': \"Parents are busy and it 's sometimes hard to get them out .\", 'label': 0, 'input_ids': [0, 398, 3714, 3229, 2363, 27863, 1380, 136, 442, 242, 7, 7941, 47, 2046, 2856, 1810, 136, 10, 5915, 111, 27863, 765, 44677, 47, 738, 136, 136, 8966, 1884, 450, 136, 442, 242, 7, 72399, 99, 17431, 221, 2, 2, 170918, 7, 621, 86352, 136, 442, 242, 7, 68018, 7941, 47, 2046, 2856, 1810, 6, 5, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n","INFO:__main__:Sample 58369 of the training set: {'premise': 'Where is art ?', 'hypothesis': 'Where and what is art ?', 'label': 1, 'input_ids': [0, 78662, 83, 4927, 705, 2, 2, 78662, 136, 2367, 83, 4927, 705, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n","INFO:__main__:Sample 13112 of the training set: {'premise': 'Alcohol and injury , as well as brief interventions , are on the list .', 'hypothesis': 'The list says alcohol and injury are negatives facing staff .', 'label': 1, 'input_ids': [0, 884, 587, 6846, 136, 182260, 6, 4, 237, 5299, 237, 59335, 113449, 7, 6, 4, 621, 98, 70, 5303, 6, 5, 2, 2, 581, 5303, 17378, 57913, 136, 182260, 621, 40907, 7, 7808, 214, 23082, 6, 5, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n","Downloading builder script: 2.95kB [00:00, 8.09MB/s]\n","2025-09-23 09:08:01.088602: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2025-09-23 09:08:01.104434: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1758618481.122534    5110 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1758618481.127938    5110 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1758618481.142997    5110 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1758618481.143021    5110 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1758618481.143025    5110 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1758618481.143029    5110 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","2025-09-23 09:08:01.147626: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","[INFO|trainer.py:1012] 2025-09-23 09:08:04,334 >> The following columns in the Training set don't have a corresponding argument in `XLMRWithEarlyExit.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `XLMRWithEarlyExit.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:2519] 2025-09-23 09:08:04,371 >> ***** Running training *****\n","[INFO|trainer.py:2520] 2025-09-23 09:08:04,371 >>   Num examples = 392,702\n","[INFO|trainer.py:2521] 2025-09-23 09:08:04,371 >>   Num Epochs = 2\n","[INFO|trainer.py:2522] 2025-09-23 09:08:04,371 >>   Instantaneous batch size per device = 128\n","[INFO|trainer.py:2525] 2025-09-23 09:08:04,371 >>   Total train batch size (w. parallel, distributed & accumulation) = 128\n","[INFO|trainer.py:2526] 2025-09-23 09:08:04,371 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:2527] 2025-09-23 09:08:04,371 >>   Total optimization steps = 6,136\n","[INFO|trainer.py:2528] 2025-09-23 09:08:04,372 >>   Number of trainable parameters = 278,048,262\n","[INFO|integration_utils.py:858] 2025-09-23 09:08:04,394 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n","\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n","\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdamianxml\u001b[0m (\u001b[33mdamianxml-uppsala-universitet\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.21.4\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250923_090830-0mb5g58c\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmisunderstood-capybara-276\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/damianxml-uppsala-universitet/huggingface\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/damianxml-uppsala-universitet/huggingface/runs/0mb5g58c\u001b[0m\n","  1% 33/6136 [00:20<59:43,  1.70it/s]Traceback (most recent call last):\n","  File \"/content/drive/MyDrive/thesis/xnli/run_xnli_eff.py\", line 501, in <module>\n","    main()\n","  File \"/content/drive/MyDrive/thesis/xnli/run_xnli_eff.py\", line 439, in main\n","    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n","                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\", line 2325, in train\n","    return inner_training_loop(\n","           ^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\", line 2674, in _inner_training_loop\n","    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n","                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\", line 4019, in training_step\n","    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\", line 4109, in compute_loss\n","    outputs = model(**inputs)\n","              ^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n","    return forward_call(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/drive/MyDrive/thesis/xnli/ee_model.py\", line 58, in forward\n","    if (entropy < self.threshold).all():\n","       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","KeyboardInterrupt\n","\u001b[1;34mwandb\u001b[0m: \n","\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mmisunderstood-capybara-276\u001b[0m at: \u001b[34m\u001b[0m\n","\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250923_090830-0mb5g58c/logs\u001b[0m\n","^C\n"]}]},{"cell_type":"code","source":["# Evaluation\n","!python /content/drive/MyDrive/thesis/xnli/restructured_run_xnli_eff.py \\\n","  --model_name_or_path /content/drive/MyDrive/thesis/xnli/distilled/l7_ft/e1 \\\n","  --language ur \\\n","  --train_language en \\\n","  --do_eval \\\n","  --per_device_train_batch_size 128 \\\n","  --learning_rate 2e-5 \\\n","  --num_train_epochs 2 \\\n","  --max_seq_length 128 \\\n","  --output_dir /content/drive/MyDrive/thesis/xnli/distilled/l7_eval/e1_ee/ur_test \\\n","  --save_steps -1 \\\n","  --early_exit False"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2jZ6W_Jk0DIQ","executionInfo":{"status":"ok","timestamp":1758618828596,"user_tz":-120,"elapsed":100991,"user":{"displayName":"Xingming Li","userId":"09069172943603800000"}},"outputId":"aad17d74-25ab-44d1-9651-ff5488271f68"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:__main__:Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","INFO:__main__:Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","average_tokens_across_devices=True,\n","batch_eval_metrics=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_persistent_workers=False,\n","dataloader_pin_memory=True,\n","dataloader_prefetch_factor=None,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","do_eval=True,\n","do_predict=False,\n","do_train=False,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_do_concat_batches=True,\n","eval_on_start=False,\n","eval_steps=None,\n","eval_strategy=IntervalStrategy.NO,\n","eval_use_gather_object=False,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=None,\n","hub_revision=None,\n","hub_strategy=HubStrategy.EVERY_SAVE,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_for_metrics=[],\n","include_inputs_for_metrics=False,\n","include_num_input_tokens_seen=no,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=2e-05,\n","length_column_name=length,\n","liger_kernel_config=None,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=/content/drive/MyDrive/thesis/xnli/distilled/l7_eval/e1_ee/ur_test/runs/Sep23_09-12-13_83d6e96eeec3,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=IntervalStrategy.STEPS,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=SchedulerType.LINEAR,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_train_epochs=2.0,\n","optim=OptimizerNames.ADAMW_TORCH_FUSED,\n","optim_args=None,\n","optim_target_modules=None,\n","output_dir=/content/drive/MyDrive/thesis/xnli/distilled/l7_eval/e1_ee/ur_test,\n","overwrite_output_dir=False,\n","parallelism_config=None,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=128,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard', 'wandb'],\n","restore_callback_states_from_checkpoint=False,\n","resume_from_checkpoint=None,\n","run_name=None,\n","save_on_each_node=False,\n","save_only_model=False,\n","save_safetensors=True,\n","save_steps=-1.0,\n","save_strategy=SaveStrategy.STEPS,\n","save_total_limit=None,\n","seed=42,\n","skip_memory_metrics=True,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torch_empty_cache_steps=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_legacy_prediction_loop=False,\n","use_liger_kernel=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","Found cached dataset xnli (/root/.cache/huggingface/datasets/xnli/ur/0.0.0/b8dd5d7af51114dbda02c0e3f6133f332186418e)\n","INFO:datasets.builder:Found cached dataset xnli (/root/.cache/huggingface/datasets/xnli/ur/0.0.0/b8dd5d7af51114dbda02c0e3f6133f332186418e)\n","[INFO|configuration_utils.py:757] 2025-09-23 09:12:16,352 >> loading configuration file /content/drive/MyDrive/thesis/xnli/distilled/l7_ft/e1/config.json\n","[INFO|configuration_utils.py:833] 2025-09-23 09:12:16,355 >> Model config XLMRobertaConfig {\n","  \"architectures\": [\n","    \"XLMRobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"dtype\": \"float32\",\n","  \"eos_token_id\": 2,\n","  \"finetuning_task\": \"xnli\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"entailment\",\n","    \"1\": \"neutral\",\n","    \"2\": \"contradiction\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"contradiction\": 2,\n","    \"entailment\": 0,\n","    \"neutral\": 1\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"xlm-roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_hidden_states\": true,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"transformers_version\": \"4.57.0.dev0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250002\n","}\n","\n","[INFO|tokenization_utils_base.py:2057] 2025-09-23 09:12:16,754 >> loading file sentencepiece.bpe.model\n","[INFO|tokenization_utils_base.py:2057] 2025-09-23 09:12:16,754 >> loading file tokenizer.json\n","[INFO|tokenization_utils_base.py:2057] 2025-09-23 09:12:16,754 >> loading file added_tokens.json\n","[INFO|tokenization_utils_base.py:2057] 2025-09-23 09:12:16,754 >> loading file special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2057] 2025-09-23 09:12:16,754 >> loading file tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2057] 2025-09-23 09:12:16,754 >> loading file chat_template.jinja\n","[INFO|modeling_utils.py:1202] 2025-09-23 09:12:18,255 >> loading weights file /content/drive/MyDrive/thesis/xnli/distilled/l7_ft/e1/model.safetensors\n","[INFO|modeling_utils.py:5572] 2025-09-23 09:12:25,955 >> All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n","\n","[INFO|modeling_utils.py:5580] 2025-09-23 09:12:25,955 >> All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at /content/drive/MyDrive/thesis/xnli/distilled/l7_ft/e1.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n","Running tokenizer on validation dataset:   0% 0/2490 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/xnli/ur/0.0.0/b8dd5d7af51114dbda02c0e3f6133f332186418e/cache-e7972b241671d960.arrow\n","INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/xnli/ur/0.0.0/b8dd5d7af51114dbda02c0e3f6133f332186418e/cache-e7972b241671d960.arrow\n","Running tokenizer on validation dataset: 100% 2490/2490 [00:00<00:00, 12393.18 examples/s]\n","2025-09-23 09:12:36.530129: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2025-09-23 09:12:36.546977: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1758618756.568484    6841 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1758618756.575443    6841 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1758618756.592278    6841 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1758618756.592305    6841 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1758618756.592308    6841 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1758618756.592311    6841 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","2025-09-23 09:12:36.597174: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","INFO:__main__:*** Evaluate ***\n","[INFO|trainer.py:1012] 2025-09-23 09:12:39,373 >> The following columns in the Evaluation set don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: premise, hypothesis. If premise, hypothesis are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:4642] 2025-09-23 09:12:39,384 >> \n","***** Running Evaluation *****\n","[INFO|trainer.py:4644] 2025-09-23 09:12:39,384 >>   Num examples = 2490\n","[INFO|trainer.py:4647] 2025-09-23 09:12:39,384 >>   Batch size = 8\n","100% 312/312 [00:54<00:00,  2.69it/s][INFO|integration_utils.py:858] 2025-09-23 09:13:42,921 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdamianxml\u001b[0m (\u001b[33mdamianxml-uppsala-universitet\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.21.4\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250923_091342-eu0j2jjh\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mzany-darkness-278\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/damianxml-uppsala-universitet/huggingface\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/damianxml-uppsala-universitet/huggingface/runs/eu0j2jjh\u001b[0m\n","100% 312/312 [01:04<00:00,  4.85it/s]\n","WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::ne encountered 1 time(s)\n","WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::cumsum encountered 1 time(s)\n","WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::add encountered 27 time(s)\n","WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::mul encountered 1 time(s)\n","WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::embedding encountered 3 time(s)\n","WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::add_ encountered 1 time(s)\n","WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::sub encountered 1 time(s)\n","WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n","WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::gelu encountered 12 time(s)\n","WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::tanh encountered 1 time(s)\n","WARNING:fvcore.nn.jit_analysis:The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n","model.roberta.encoder.layer.0.attention.self.dropout, model.roberta.encoder.layer.1.attention.self.dropout, model.roberta.encoder.layer.10.attention.self.dropout, model.roberta.encoder.layer.11.attention.self.dropout, model.roberta.encoder.layer.2.attention.self.dropout, model.roberta.encoder.layer.3.attention.self.dropout, model.roberta.encoder.layer.4.attention.self.dropout, model.roberta.encoder.layer.5.attention.self.dropout, model.roberta.encoder.layer.6.attention.self.dropout, model.roberta.encoder.layer.7.attention.self.dropout, model.roberta.encoder.layer.8.attention.self.dropout, model.roberta.encoder.layer.9.attention.self.dropout\n","***** eval metrics *****\n","  avg_inference_time_sec      =     0.0088\n","  eval_accuracy               =     0.6647\n","  eval_loss                   =     0.8172\n","  eval_model_preparation_time =     0.0029\n","  eval_runtime                = 0:01:03.53\n","  eval_samples                =       2490\n","  eval_samples_per_second     =     39.194\n","  eval_steps_per_second       =      4.911\n","  flops_giga                  =      10.88\n","  peak_memory_GB              =      1.053\n","\u001b[1;34mwandb\u001b[0m: \n","\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mzany-darkness-278\u001b[0m at: \u001b[34m\u001b[0m\n","\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250923_091342-eu0j2jjh/logs\u001b[0m\n"]}]}]}